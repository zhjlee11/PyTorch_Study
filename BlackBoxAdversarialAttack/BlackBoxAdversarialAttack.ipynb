{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Adversarial Attack (in Black-Box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "\n",
    "import random\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 사용 기기 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) 하이퍼 파라미터 셋팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS     = 300\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "MODEL_PATH = './model/PreTrained ResNet/preTrainedResNet_mnist.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) 공격할 모델 가져오기\n",
    "---------------------------------------------------------------------------\n",
    "CIFAR-10으로 미리 학습된 모델을 가져올 것이다.\n",
    "\n",
    "물론, 이 모델 그대로 FGSM 공격 이미지를 생성할 수는 있지만,\n",
    "BlackBox인 상황이라고 가정하고 진행할 것이기에,\n",
    "\n",
    "모방 모델을 만들어 FGSM 공격 이미지를 생성할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(16, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(32, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(64, 2, stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "\n",
    "    def _make_layer(self, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(BasicBlock(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, 7)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "willBeHackedModel = torch.load(MODEL_PATH)\n",
    "willBeHackedModel = willBeHackedModel.to(DEVICE)\n",
    "willBeHackedModel.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) 데이터셋 가져오기\n",
    "------------------------------------------------------------\n",
    "모방 모델 또한 CIFAR-10으로 학습할 예정이다. 같은 라벨 클래스를 가진 데이터셋이 존재하지 않아 같은 것으로 학습하겠다.\n",
    "어차피 같은 것이든 아니든, 본 CIFAR-10의 y값은 쓰지 않을 것이고, 기존 모델의 분류값을 y값으로 사용할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cifar_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('E:\\Dataset\\mnist',\n",
    "                   train=True,\n",
    "                   download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "    batch_size=1, shuffle=True)\n",
    "test_cifar_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('E:\\Dataset\\mnist',\n",
    "                   train=False, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "    batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (data, target) in enumerate(train_cifar_loader):\n",
    "    print(target.data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset): \n",
    "  def __init__(self, cifar, _model):\n",
    "    self.x_data = []\n",
    "    self.y_data = None\n",
    "    self.mix = {}\n",
    "    self.model = _model\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(cifar):\n",
    "        self.x_data.append(data)\n",
    "    random.shuffle(self.x_data)\n",
    "\n",
    "  def __len__(self): \n",
    "    return len(self.x_data)\n",
    "\n",
    "  def __getitem__(self, idx): \n",
    "    x = self.x_data[idx]\n",
    "    y = self.model(x).max(1, keepdim=True)[1].item()\n",
    "    return x.squeeze(0), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(CustomDataset(train_cifar_loader, willBeHackedModel), batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(CustomDataset(test_cifar_loader, willBeHackedModel), batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6) 모방 모델 만들기\n",
    "------------------------------------------------------------\n",
    "모방 모델은 ResNet으로 만들 것이다.\n",
    "어차피 실제로 할 때도 머신러닝 지식에 의해 개발자가 직접 선정한다고 하니...\n",
    "\n",
    "실제로 할 때는 모델을 정확하게 맞출 수 없음으로, 약간 다른 점을 주기 위해서, BasicBlock를 한 층 더 늘렸다. 이게 성능의 어떤 영향을 주든 BlackBox를 위한 코드니 차별점을 주기 위해 어쩔 수 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImitationResNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ImitationResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(16, 2, stride=1)\n",
    "        #self.layer2 = self._make_layer(32, 2, stride=2)\n",
    "        #self.layer3 = self._make_layer(48, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(64, 2, stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "\n",
    "    def _make_layer(self, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(BasicBlock(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.layer1(out)\n",
    "        #out = self.layer2(out)\n",
    "        #out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 14)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImitationModel = ImitationResNet().to(DEVICE)\n",
    "optimizer = optim.SGD(ImitationModel.parameters(), lr=0.1,\n",
    "                      momentum=0.9, weight_decay=0.0005)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    bar_total = tqdm_notebook(enumerate(train_loader), total=len(train_loader))\n",
    "    for batch_idx, (data, target) in bar_total:\n",
    "        bar_total.set_description(\"{0}번째 학습 - {1}번 배치\".format(epoch, batch_idx))\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        #print(output.data)\n",
    "        #output = output.max(1, keepdim=True)[1]\n",
    "        #target = torch.max(target, 2)[1].squeeze(1)\n",
    "        #print(output.data)\n",
    "        #print(target.data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    lastoutput = None\n",
    "    lasttarget = None\n",
    "    with torch.no_grad():\n",
    "        bar_total = tqdm_notebook(enumerate(test_loader), total=len(test_loader))\n",
    "        for batch_idx, (data, target) in bar_total:\n",
    "            bar_total.set_description(\"테스트 중 - {0}번 배치\".format(batch_idx))\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "            #target = torch.max(target, 2)[1].squeeze(1)\n",
    "            lastoutput = output\n",
    "            lasttarget = target\n",
    "            # 배치 오차를 합산\n",
    "            \n",
    "            loss = F.cross_entropy(output.float(), target.long(), reduction='sum')\n",
    "            test_loss += loss.item()\n",
    "            # 가장 높은 값을 가진 인덱스가 바로 예측값\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy, lastoutput.data, pred.data, lasttarget.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(path, epoch, model, optimizer, scheduler):\n",
    "    state = {\n",
    "        'Epoch': epoch,\n",
    "        'State_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'scheduler': scheduler.state_dict()\n",
    "    }\n",
    "    torch.save(state, path)\n",
    "    \n",
    "def load_checkpoint(path, model, optimitzer, scheduler):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['State_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "    epoch = int(checkpoint['Epoch'])\n",
    "    return model, optimizer, epoch\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (7) FGSM 이미지 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(image, epsilon, gradient):\n",
    "    sign_gradient = gradient.sign()\n",
    "    perturbed_image = image + epsilon * sign_gradient\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    return perturbed_image\n",
    "\n",
    "def getPertubedImage(img_tensor, target, epsilon, model):\n",
    "    img_tensor = img_tensor#.unsqueeze(0)\n",
    "    img_tensor.requires_grad_(True)\n",
    "    output = model(img_tensor)\n",
    "    loss = F.nll_loss(output, target) \n",
    "    \n",
    "    model.zero_grad()\n",
    "    \n",
    "    loss.requires_grad_(True)\n",
    "    loss.backward()\n",
    "    \n",
    "    gradient = img_tensor.grad.data\n",
    "    perturbed_data = fgsm_attack(img_tensor, epsilon, gradient)\n",
    "    return perturbed_data#.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = (img+1)/2    \n",
    "    img = img.squeeze()\n",
    "    np_img = img.detach().numpy()\n",
    "    plt.imshow(np_img, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_FGSM_attack(model, willBeHackedModel, dataloader, epsilon=0.07) :\n",
    "    success_num=0\n",
    "    bar_total = tqdm_notebook(enumerate(dataloader), total=len(dataloader))\n",
    "    for batch_idx, (data, target) in bar_total:       \n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        realTarget = target\n",
    "        perturbed_image = getPertubedImage(data, realTarget, epsilon, model)\n",
    "        if target.item() != willBeHackedModel(data).max(1, keepdim=True)[1].item() or target.item() != model(data).max(1, keepdim=True)[1].item() :\n",
    "            continue\n",
    "        #if willBeHackedModel(perturbed_image).max(1, keepdim=True)[1].item() != model(perturbed_image).max(1, keepdim=True)[1].item() :\n",
    "            #continue\n",
    "        #if target.item() == willBeHackedModel(perturbed_image).max(1, keepdim=True)[1].item() or target.item() == model(perturbed_image).max(1, keepdim=True)[1].item() :\n",
    "            #continue\n",
    "            \n",
    "        imitaionTarget = model(data).max(1, keepdim=True)[1].squeeze(0)\n",
    "        perturbed_original_image = getPertubedImage(data, target, epsilon, willBeHackedModel)\n",
    "        \n",
    "        if willBeHackedModel(perturbed_original_image).max(1, keepdim=False)[1].item() != willBeHackedModel(perturbed_image).max(1, keepdim=False)[1].item():\n",
    "            continue\n",
    "        if model(perturbed_original_image).max(1, keepdim=False)[1].item() != model(perturbed_image).max(1, keepdim=False)[1].item():\n",
    "            continue\n",
    "            \n",
    "            \n",
    "        imshow(data[0])\n",
    "        print(\"모범 답안 : \" + str(target.item()))\n",
    "        print(\"----------------------------------------------------------------------------------------------------\")\n",
    "        print(\"원본 이미지 원본 모델 예측 : \" + str(willBeHackedModel(data).max(1, keepdim=False)[1].item()))\n",
    "        print(\"원본 이미지 모방 모델 예측 : \" + str(model(data).max(1, keepdim=False)[1].item()))\n",
    "        print(\"----------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "        realTarget = target\n",
    "        perturbed_image = getPertubedImage(data, realTarget, epsilon, model)\n",
    "        imshow(perturbed_image)\n",
    "        #pic = np.transpose(perturbed_image.detach().numpy()[0], (1, 2, 0))\n",
    "        #plt.imshow(pic)\n",
    "        #plt.show()\n",
    "        print(\"왜곡 이미지(타겟 = real) 원본 모델 예측 : \" + str(willBeHackedModel(perturbed_image).max(1, keepdim=False)[1].item()))\n",
    "        print(\"왜곡 이미지(타겟 = real) 모방 모델 예측 : \" + str(model(perturbed_image).max(1, keepdim=False)[1].item()))\n",
    "        print(\"----------------------------------------------------------------------------------------------------\")\n",
    "        originalTarget = willBeHackedModel(data).max(1, keepdim=True)[1].squeeze(0)\n",
    "        perturbed_image = getPertubedImage(data, originalTarget, epsilon, model)\n",
    "        print(\"왜곡 이미지(타겟 = 원본 모델) 원본 모델 예측 : \" + str(willBeHackedModel(perturbed_image).max(1, keepdim=False)[1].item()))\n",
    "        print(\"왜곡 이미지(타겟 = 원본 모델) 모방 모델 예측 : \" + str(model(perturbed_image).max(1, keepdim=False)[1].item()))\n",
    "        print(\"----------------------------------------------------------------------------------------------------\")\n",
    "        imitaionTarget = model(data).max(1, keepdim=True)[1].squeeze(0)\n",
    "        perturbed_image = getPertubedImage(data, imitaionTarget, epsilon, model)\n",
    "        print(\"왜곡 이미지(타겟 = 모방 모델) 원본 모델 예측 : \" + str(willBeHackedModel(perturbed_image).max(1, keepdim=False)[1].item()))\n",
    "        print(\"왜곡 이미지(타겟 = 모방 모델) 모방 모델 예측 : \" + str(model(perturbed_image).max(1, keepdim=False)[1].item()))\n",
    "        print(\"----------------------------------------------------------------------------------------------------\")\n",
    "        realTarget = target\n",
    "        perturbed_image = getPertubedImage(data, realTarget, epsilon, willBeHackedModel)\n",
    "        print(\"왜곡 이미지(모델 = 원본) 원본 모델 예측 : \" + str(willBeHackedModel(perturbed_image).max(1, keepdim=False)[1].item()))\n",
    "        print(\"왜곡 이미지(모델 = 원본) 모방 모델 예측 : \" + str(model(perturbed_image).max(1, keepdim=False)[1].item()))\n",
    "        break\n",
    "    #print(str(success_num) + \"/\" + str(len(dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_num_attack_succeess(model, willBeHackedModel, dataloader, epsilon=0.07) :\n",
    "    success_num=0\n",
    "    total_correct=0\n",
    "    bar_total = tqdm_notebook(enumerate(dataloader), total=len(dataloader))\n",
    "    for batch_idx, (data, target) in bar_total:    \n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        bar_total.set_description(\"공격 성공 : {0}/{1}\".format(success_num, total_correct))\n",
    "        realTarget = target\n",
    "        perturbed_image = getPertubedImage(data, realTarget, epsilon, model)\n",
    "        \n",
    "        #원본 이미지에서 정답을 맞췄는가. 틀렸으면 논외로 성공도 실패도 아님.\n",
    "        if target.item() != willBeHackedModel(data).max(1, keepdim=True)[1].item() or target.item() != model(data).max(1, keepdim=True)[1].item() :\n",
    "            continue\n",
    "        else :\n",
    "            total_correct += 1\n",
    "\n",
    "        #답안이 실제랑 같으면 실패\n",
    "        if target.item() == willBeHackedModel(perturbed_image).max(1, keepdim=True)[1].item():\n",
    "            continue\n",
    "        \n",
    "        #원본 모델로 FGSM 공격 할때와 같은 결과가 나오면 공격 성공\n",
    "        perturbed_original_image = getPertubedImage(data, target, epsilon, willBeHackedModel)\n",
    "        if willBeHackedModel(perturbed_original_image).max(1, keepdim=False)[1].item() != willBeHackedModel(perturbed_image).max(1, keepdim=False)[1].item():\n",
    "            continue\n",
    "        if model(perturbed_original_image).max(1, keepdim=False)[1].item() != model(perturbed_image).max(1, keepdim=False)[1].item():\n",
    "            continue\n",
    "            \n",
    "        #imshow(data[0])\n",
    "        #print(\"모범 답안 : \" + str(target.item()))\n",
    "        #print(\"----------------------------------------------------------------------------------------------------\")\n",
    "        #print(\"원본 이미지 원본 모델 예측 : \" + str(willBeHackedModel(data).max(1, keepdim=False)[1].item()))\n",
    "        #print(\"원본 이미지 모방 모델 예측 : \" + str(model(data).max(1, keepdim=False)[1].item()))\n",
    "        #print(\"----------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "        realTarget = target\n",
    "        perturbed_image = getPertubedImage(data, realTarget, epsilon, model)\n",
    "        #pic = np.transpose(perturbed_image.detach().numpy()[0], (1, 2, 0))\n",
    "        #plt.imshow(pic)\n",
    "        #plt.show()\n",
    "        #print(\"왜곡 이미지(타겟 = real) 원본 모델 예측 : \" + str(willBeHackedModel(perturbed_image).max(1, keepdim=False)[1].item()))\n",
    "        #print(\"왜곡 이미지(타겟 = real) 모방 모델 예측 : \" + str(model(perturbed_image).max(1, keepdim=False)[1].item()))\n",
    "        #print(\"----------------------------------------------------------------------------------------------------\")\n",
    "        originalTarget = willBeHackedModel(data).max(1, keepdim=True)[1].squeeze(0)\n",
    "        perturbed_image = getPertubedImage(data, originalTarget, epsilon, model)\n",
    "        \n",
    "        #print(\"왜곡 이미지(타겟 = 원본 모델) 원본 모델 예측 : \" + str(willBeHackedModel(perturbed_image).max(1, keepdim=False)[1].item()))\n",
    "        #print(\"왜곡 이미지(타겟 = 원본 모델) 모방 모델 예측 : \" + str(model(perturbed_image).max(1, keepdim=False)[1].item()))\n",
    "        #print(\"----------------------------------------------------------------------------------------------------\")\n",
    "        imitaionTarget = model(data).max(1, keepdim=True)[1].squeeze(0)\n",
    "        perturbed_image = getPertubedImage(data, imitaionTarget, epsilon, model)\n",
    "        #print(\"왜곡 이미지(타겟 = 모방 모델) 원본 모델 예측 : \" + str(willBeHackedModel(perturbed_image).max(1, keepdim=False)[1].item()))\n",
    "        #print(\"왜곡 이미지(타겟 = 모방 모델) 모방 모델 예측 : \" + str(model(perturbed_image).max(1, keepdim=False)[1].item()))\n",
    "        #print(\"----------------------------------------------------------------------------------------------------\")\n",
    "        imitaionTarget = model(data).max(1, keepdim=True)[1].squeeze(0)\n",
    "        perturbed_original_image = getPertubedImage(data, imitaionTarget, epsilon, willBeHackedModel)\n",
    "        #print(\"왜곡 이미지(모델 = 원본) 원본 모델 예측 : \" + str(willBeHackedModel(perturbed_image).max(1, keepdim=False)[1].item()))\n",
    "        #print(\"왜곡 이미지(모델 = 원본) 모방 모델 예측 : \" + str(model(perturbed_image).max(1, keepdim=False)[1].item()))\n",
    "        success_num+=1\n",
    "    print(\"공격 성공 횟수 : \" + str(success_num) + \"/\" + str(total_correct)) \n",
    "    print(\"공격 성공률 (백분율) : {0}%\".format(success_num*100/total_correct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (8) 학습 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3cd113b7b8247839c482de2ae80c22f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모범 답안 : 2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "원본 이미지 원본 모델 예측 : 2\n",
      "원본 이미지 모방 모델 예측 : 2\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADA9JREFUeJzt3V+IXOUdxvHnqdqbGCESEoOaaiXUFkEtixQMopaILcGooCRXKa1dQQMqvah4oxAEKTW1eCGsGIygSQW1Bi31TyhJCkWySojRNCqSmjQxaUgkES9E8+vFnsgad87Mzpw/s/l9P7DMzHln5vxyss+ec+ad876OCAHI53ttFwCgHYQfSIrwA0kRfiApwg8kRfiBpAg/kBThB5Ii/EBSZza5Mtt8nRCoWUS4l+cNtOe3faPt3bY/sn3/IO8FoFnu97v9ts+Q9IGkJZL2SdomaUVEvF/yGvb8QM2a2PNfJemjiPg4Ir6UtEHSsgHeD0CDBgn/+ZL2Tnq8r1j2LbZHbY/bHh9gXQAqNsgHflMdWnznsD4ixiSNSRz2A8NkkD3/PkkXTnp8gaT9g5UDoCmDhH+bpEW2L7b9fUnLJW2spiwAdev7sD8ivrK9StJrks6QtDYi3qusMgC16rurr6+Vcc4P1K6RL/kAmLkIP5AU4QeSIvxAUoQfSIrwA0kRfiApwg8kRfiBpAg/kBThB5Ii/EBShB9IqtGhu1GPTZs2dWy7/vrrG6xkes4555zS9uPHjzdUSU7s+YGkCD+QFOEHkiL8QFKEH0iK8ANJEX4gKUbvnQG2bt1a2r548eKGKmnWnDlzSts/++yzhiqZWRi9F0Apwg8kRfiBpAg/kBThB5Ii/EBShB9IaqDr+W3vkXRc0teSvoqIkSqKwrdt3769tH2Qfv7Dhw/3/VpJmjt37kCvL3P06NHSdrun7mx0UMVgHtdFxGC/QQAax2E/kNSg4Q9Jr9t+2/ZoFQUBaMagh/1XR8R+2/MkvWH73xGxZfITij8K/GEAhsxAe/6I2F/cHpL0kqSrpnjOWESM8GEgMFz6Dr/tWbZnn7wv6QZJO6sqDEC9Bjnsny/ppaK75UxJz0XE3yupCkDtuJ7/NHDTTTd1bNu4cWOt677vvvtK29esWVPbuunnnxrX8wMoRfiBpAg/kBThB5Ii/EBShB9Iiq4+DGT27Nml7ceOHatt3XT1TY2uPgClCD+QFOEHkiL8QFKEH0iK8ANJEX4gqSpG70Vid9xxR9sloE/s+YGkCD+QFOEHkiL8QFKEH0iK8ANJEX4gKcIPJEX4gaQIP5AU4QeSIvxAUoQfSIrwA0kRfiCpruG3vdb2Ids7Jy071/Ybtj8sbufUWyaAqvWy539a0o2nLLtf0qaIWCRpU/EYwAzSNfwRsUXSkVMWL5O0rri/TtLNFdcFoGb9nvPPj4gDklTczquuJABNqH0MP9ujkkbrXg+A6el3z3/Q9gJJKm4PdXpiRIxFxEhEjPS5LgA16Df8GyWtLO6vlPRyNeUAaEovXX3rJf1L0o9s77P9G0mPSFpi+0NJS4rHAGYQR0RzK7ObWxka0eTvz6nsnqahTycietowfMMPSIrwA0kRfiApwg8kRfiBpAg/kBThB5Ii/EBShB9IivADSRF+ICnCDyRF+IGkCD+QVO3DeGFma/OS3dWrV7e27gzY8wNJEX4gKcIPJEX4gaQIP5AU4QeSIvxAUgzdfRq47rrrOrYtX7689LW33nprafvcuXP7qqkKDM3dH4buBlCK8ANJEX4gKcIPJEX4gaQIP5AU4QeS6no9v+21kpZKOhQRlxXLHpL0W0n/K572QET8ra4iZ7r58+eXtn/66acNVTKzDPodlLLtet555w303tu2bSttv+uuu0rbx8fHB1p/FXrZ8z8t6cYplv8pIq4ofgg+MMN0DX9EbJF0pIFaADRokHP+VbZ32F5re05lFQFoRL/hf0LSJZKukHRA0qOdnmh71Pa47fZPcgB8o6/wR8TBiPg6Ik5IelLSVSXPHYuIkYgY6bdIANXrK/y2F0x6eIukndWUA6ApvXT1rZd0raS5tvdJelDStbavkBSS9ki6s8YaAdSA6/krcM0115S2b968uaFKMFOsWLGiY9uGDRsGem+u5wdQivADSRF+ICnCDyRF+IGkCD+QFF19FWhzGuuZbOHChaXte/fubaiS6ev2f75jx47S9ssvv7zKcr6Frj4ApQg/kBThB5Ii/EBShB9IivADSRF+IKmu1/OjfevXry9tL7s8tG2n6zTbp8O/iz0/kBThB5Ii/EBShB9IivADSRF+ICnCDyTF9fw9avOa/W59ym3Wdumll5a27969u6FKcBLX8wMoRfiBpAg/kBThB5Ii/EBShB9IivADSXW9nt/2hZKekXSepBOSxiLiz7bPlfQXSRdJ2iPp9og4Wl+pp6+xsbHS9jb78WfNmlXa/sUXXzRUCarWy57/K0m/i4gfS/qZpLtt/0TS/ZI2RcQiSZuKxwBmiK7hj4gDEfFOcf+4pF2Szpe0TNK64mnrJN1cV5EAqjetc37bF0m6UtJbkuZHxAFp4g+EpHlVFwegPj2P4Wf7bEkvSLo3Io71OoaZ7VFJo/2VB6AuPe35bZ+lieA/GxEvFosP2l5QtC+QdGiq10bEWESMRMRIFQUDqEbX8HtiF/+UpF0RsWZS00ZJK4v7KyW9XH15AOrS9ZJe24slbZX0ria6+iTpAU2c9z8vaaGkTyTdFhFHurzXjL2k95577unY9thjjzVYSbWWLFlS2v7mm282VAmq0uslvV3P+SPin5I6vdnPp1MUgOHBN/yApAg/kBThB5Ii/EBShB9IivADSTF0dwUef/zx0vZVq1bVuv6lS5d2bHv11VdrXTeGD0N3AyhF+IGkCD+QFOEHkiL8QFKEH0iK8ANJ0c8PnGbo5wdQivADSRF+ICnCDyRF+IGkCD+QFOEHkiL8QFKEH0iK8ANJEX4gKcIPJEX4gaQIP5AU4QeS6hp+2xfa/oftXbbfs31Psfwh2/+1vb34+WX95QKoStfBPGwvkLQgIt6xPVvS25JulnS7pM8j4o89r4zBPIDa9TqYx5k9vNEBSQeK+8dt75J0/mDlAWjbtM75bV8k6UpJbxWLVtneYXut7TkdXjNqe9z2+ECVAqhUz2P42T5b0mZJD0fEi7bnSzosKSSt1sSpwa+7vAeH/UDNej3s7yn8ts+S9Iqk1yJizRTtF0l6JSIu6/I+hB+oWWUDeNq2pKck7Zoc/OKDwJNukbRzukUCaE8vn/YvlrRV0ruSThSLH5C0QtIVmjjs3yPpzuLDwbL3Ys8P1KzSw/6qEH6gfozbD6AU4QeSIvxAUoQfSIrwA0kRfiApwg8kRfiBpAg/kBThB5Ii/EBShB9IivADSRF+IKmuA3hW7LCk/0x6PLdYNoyGtbZhrUuitn5VWdsPen1io9fzf2fl9nhEjLRWQIlhrW1Y65KorV9t1cZhP5AU4QeSajv8Yy2vv8yw1jasdUnU1q9Wamv1nB9Ae9re8wNoSSvht32j7d22P7J9fxs1dGJ7j+13i5mHW51irJgG7ZDtnZOWnWv7DdsfFrdTTpPWUm1DMXNzyczSrW67YZvxuvHDfttnSPpA0hJJ+yRtk7QiIt5vtJAObO+RNBIRrfcJ275G0ueSnjk5G5LtP0g6EhGPFH8450TE74ektoc0zZmba6qt08zSv1KL267KGa+r0Mae/ypJH0XExxHxpaQNkpa1UMfQi4gtko6csniZpHXF/XWa+OVpXIfahkJEHIiId4r7xyWdnFm61W1XUlcr2gj/+ZL2Tnq8T8M15XdIet3227ZH2y5mCvNPzoxU3M5ruZ5TdZ25uUmnzCw9NNuunxmvq9ZG+KeaTWSYuhyujoifSvqFpLuLw1v05glJl2hiGrcDkh5ts5hiZukXJN0bEcfarGWyKepqZbu1Ef59ki6c9PgCSftbqGNKEbG/uD0k6SVNnKYMk4MnJ0ktbg+1XM83IuJgRHwdESckPakWt10xs/QLkp6NiBeLxa1vu6nqamu7tRH+bZIW2b7Y9vclLZe0sYU6vsP2rOKDGNmeJekGDd/swxslrSzur5T0cou1fMuwzNzcaWZptbzthm3G61a+5FN0ZTwm6QxJayPi4caLmILtH2piby9NXPH4XJu12V4v6VpNXPV1UNKDkv4q6XlJCyV9Ium2iGj8g7cOtV2rac7cXFNtnWaWfkstbrsqZ7yupB6+4QfkxDf8gKQIP5AU4QeSIvxAUoQfSIrwA0kRfiApwg8k9X8IA7nMBrl1QAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "왜곡 이미지(타겟 = real) 원본 모델 예측 : 1\n",
      "왜곡 이미지(타겟 = real) 모방 모델 예측 : 8\n",
      "----------------------------------------------------------------------------------------------------\n",
      "왜곡 이미지(타겟 = 원본 모델) 원본 모델 예측 : 1\n",
      "왜곡 이미지(타겟 = 원본 모델) 모방 모델 예측 : 8\n",
      "----------------------------------------------------------------------------------------------------\n",
      "왜곡 이미지(타겟 = 모방 모델) 원본 모델 예측 : 1\n",
      "왜곡 이미지(타겟 = 모방 모델) 모방 모델 예측 : 8\n",
      "----------------------------------------------------------------------------------------------------\n",
      "왜곡 이미지(모델 = 원본) 원본 모델 예측 : 1\n",
      "왜곡 이미지(모델 = 원본) 모방 모델 예측 : 8\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[2] Test Loss: 0.1464, Accuracy: 95.60%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "646011b2c84849a485f2c64666fe8944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=469.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-158-e4a58725b0c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malready\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mImitationModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_o\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_p\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mImitationModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mclear_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-151-6e245607b7b4>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, optimizer, epoch)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;31m#print(target.data)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \"\"\"\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "already=1\n",
    "for epoch in range(already, EPOCHS + 1):\n",
    "    scheduler.step()\n",
    "    train(ImitationModel, train_loader, optimizer, epoch)\n",
    "    test_loss, test_accuracy, _o, _p, _t = evaluate(ImitationModel, test_loader)\n",
    "    clear_output(wait=True)\n",
    "    #print(\"아웃풋 : \" + str(_o))\n",
    "    #print(\"모방 모델의 답안 : \" + str(_p))\n",
    "    #print(\"원본 모델의 답안 : \" + str(_t))\n",
    "    test_FGSM_attack(ImitationModel, willBeHackedModel, test_cifar_loader)\n",
    "    print(\"----------------------------------------------------------------------------------------------------\")\n",
    "    print('[{}] Test Loss: {:.4f}, Accuracy: {:.2f}%'.format(\n",
    "          epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LHZ\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "915635be9e204788b4c6045979bb3001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모범 답안 : 5\n",
      "----------------------------------------------------------------------------------------------------\n",
      "원본 이미지 원본 모델 예측 : 5\n",
      "원본 이미지 모방 모델 예측 : 5\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "왜곡 이미지(타겟 = real) 원본 모델 예측 : 1\n",
      "왜곡 이미지(타겟 = real) 모방 모델 예측 : 2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "왜곡 이미지(타겟 = 원본 모델) 원본 모델 예측 : 1\n",
      "왜곡 이미지(타겟 = 원본 모델) 모방 모델 예측 : 2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "왜곡 이미지(타겟 = 모방 모델) 원본 모델 예측 : 1\n",
      "왜곡 이미지(타겟 = 모방 모델) 모방 모델 예측 : 2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "왜곡 이미지(모델 = 원본) 원본 모델 예측 : 1\n",
      "왜곡 이미지(모델 = 원본) 모방 모델 예측 : 2\n"
     ]
    }
   ],
   "source": [
    "test_FGSM_attack(ImitationModel, willBeHackedModel, test_cifar_loader, epsilon=0.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LHZ\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1233f0512101412aba822ef82312dbba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "공격 성공 횟수 : 2168/2262\n",
      "공격 성공률 (백분율) : 95.84438549955792%\n"
     ]
    }
   ],
   "source": [
    "get_num_attack_succeess(ImitationModel, willBeHackedModel, test_cifar_loader, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LHZ\\anaconda3\\lib\\site-packages\\torch\\serialization.py:402: UserWarning: Couldn't retrieve source code for container of type ImitationResNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\LHZ\\anaconda3\\lib\\site-packages\\torch\\serialization.py:402: UserWarning: Couldn't retrieve source code for container of type BasicBlock. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, './model/Attack Model/model_3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_checkpoint(\"checkpoints/model_1/checkpoints3.tar\", already, model, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LHZ\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83e00e9f5cf14ec78ee6925155f9a511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=79.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3.7169133827209473,\n",
       " 65.56,\n",
       " tensor([[ -1.3295,  -8.1340, -13.4282,  -5.3378,   5.5988,   5.3002, -11.8622,\n",
       "           43.7479,  -6.3951,  -8.1678],\n",
       "         [ -9.6390,  -5.4726,  -5.8292,   9.4391,  -3.4087,  26.4297,  -5.8986,\n",
       "           12.8656, -12.2009,  -6.2901],\n",
       "         [ -2.3526,   3.7780,  -6.7665,  -1.7601,  -8.3274,  -5.3754,  -6.2733,\n",
       "           -0.8379,   9.4810,  18.4347],\n",
       "         [ -1.9152,  14.9927,  -5.6941,   2.5607,  -8.3189,  -0.5827,   1.8324,\n",
       "           -6.6908,   6.9678,  -3.1544],\n",
       "         [  6.8326,   2.6636,   6.0283,  -3.5994,  -5.0497,  -8.0440,  -3.6280,\n",
       "           -2.1835,  13.7585,  -6.7769],\n",
       "         [-10.1441,  11.5962,  -4.4965,  -0.2949,  -1.7003,   2.7460,  -0.0768,\n",
       "           -7.5410,  -8.6675,  18.5780],\n",
       "         [-11.3807, -15.3519, -12.4055,  24.9558,   2.2705,  32.0103,   2.7885,\n",
       "            5.3101, -18.3358,  -9.8681],\n",
       "         [ 27.6686,  -5.6602,  -0.4504,   6.8777,  -0.7883,  -9.7300,  -5.5564,\n",
       "           -3.4166,  -2.9285,  -6.0172],\n",
       "         [ -8.2508,  17.9800,  -0.9673,   0.1278, -12.4506,  17.2123,  -4.8864,\n",
       "           -7.8817,  -9.7573,   8.8712],\n",
       "         [-17.4021, -20.4770,  -9.8746,  27.9118,   1.0169,  56.5916, -11.5248,\n",
       "            9.6808, -21.6649, -14.2696],\n",
       "         [  2.2716,  -2.5667,  -2.0095,   6.0550,  -1.9338,   1.5492,  -4.0636,\n",
       "            7.4601,  -2.8999,  -3.8644],\n",
       "         [  2.2563,   2.7499,  -3.0485,  10.4410,   3.7708,  -1.1624,  -4.8525,\n",
       "           -6.7647,  -1.4267,  -1.9647],\n",
       "         [-13.7299, -18.3445, -15.0210,   9.4089,   8.1491,  36.2589, -11.1201,\n",
       "           30.3268, -16.4524,  -9.4865],\n",
       "         [ -9.1178,  -9.6342,  -2.0514,   7.5296,   9.3743,   7.1563,  -1.4103,\n",
       "           15.5265,  -9.0789,  -8.3000],\n",
       "         [ -2.5018,  -4.3710,  -2.6966,   6.0608,   4.5541,   2.6295,   4.5244,\n",
       "            1.4031,  -4.7426,  -4.8627],\n",
       "         [ -1.3071,  30.9449,  -3.9548,   0.2105, -10.8506,   5.2409,  -5.1687,\n",
       "          -10.3592,  -1.2272,  -3.5337]]),\n",
       " tensor([[7],\n",
       "         [5],\n",
       "         [9],\n",
       "         [1],\n",
       "         [8],\n",
       "         [9],\n",
       "         [5],\n",
       "         [0],\n",
       "         [1],\n",
       "         [5],\n",
       "         [7],\n",
       "         [3],\n",
       "         [5],\n",
       "         [7],\n",
       "         [3],\n",
       "         [1]]),\n",
       " tensor([7, 5, 9, 1, 8, 9, 6, 0, 9, 5, 3, 9, 7, 5, 6, 1]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_m, _o, _e = load_checkpoint(\"checkpoints/model_1/checkpoints1.tar\", ImitationResNet(), optim.SGD(model.parameters(), lr=0.1,\n",
    "                      momentum=0.9, weight_decay=0.0005), optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
