{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Adversarial Attack (in Black-Box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "\n",
    "import random\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 사용 기기 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) 하이퍼 파라미터 셋팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS     = 300\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "MODEL_PATH = './model/PreTrained ResNet/preTrainedResNet_mnist.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) 공격할 모델 가져오기\n",
    "---------------------------------------------------------------------------\n",
    "MNIST으로 미리 학습된 모델을 가져올 것이다.\n",
    "\n",
    "물론, 이 모델 그대로 FGSM 공격 이미지를 생성할 수는 있지만,\n",
    "BlackBox인 상황이라고 가정하고 진행할 것이기에,\n",
    "\n",
    "모방 모델을 만들어 FGSM 공격 이미지를 생성할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(16, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(32, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(64, 2, stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "\n",
    "    def _make_layer(self, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(BasicBlock(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, 7)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "willBeHackedModel = torch.load(MODEL_PATH)\n",
    "willBeHackedModel = willBeHackedModel.to(DEVICE)\n",
    "willBeHackedModel.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) 데이터셋 가져오기\n",
    "------------------------------------------------------------\n",
    "모방 모델 또한 MNIST으로 학습할 예정이다. 같은 라벨 클래스를 가진 데이터셋이 존재하지 않아 같은 것으로 학습하겠다.\n",
    "어차피 같은 것이든 아니든, 본 MNIST의 y값은 쓰지 않을 것이고, 기존 모델의 분류값을 y값으로 사용할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cifar_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('E:\\Dataset\\mnist',\n",
    "                   train=True,\n",
    "                   download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "    batch_size=1, shuffle=True)\n",
    "test_cifar_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('E:\\Dataset\\mnist',\n",
    "                   train=False, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "    batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (data, target) in enumerate(train_cifar_loader):\n",
    "    print(target.data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset): \n",
    "  def __init__(self, cifar, _model):\n",
    "    self.x_data = []\n",
    "    self.y_data = None\n",
    "    self.mix = {}\n",
    "    self.model = _model\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(cifar):\n",
    "        self.x_data.append(data)\n",
    "    random.shuffle(self.x_data)\n",
    "\n",
    "  def __len__(self): \n",
    "    return len(self.x_data)\n",
    "\n",
    "  def __getitem__(self, idx): \n",
    "    x = self.x_data[idx]\n",
    "    y = self.model(x).max(1, keepdim=True)[1].item()\n",
    "    return x.squeeze(0), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(CustomDataset(train_cifar_loader, willBeHackedModel), batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(CustomDataset(test_cifar_loader, willBeHackedModel), batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6) 모방 모델 만들기\n",
    "------------------------------------------------------------\n",
    "모방 모델은 ResNet으로 만들 것이다.\n",
    "어차피 실제로 할 때도 머신러닝 지식에 의해 개발자가 직접 선정한다고 하니...\n",
    "\n",
    "실제로 할 때는 모델을 정확하게 맞출 수 없음으로, 약간 다른 점을 주기 위해서, BasicBlock를 한 층 줄였다. 이게 성능의 어떤 영향을 주든 BlackBox를 위한 코드니 차별점을 주기 위해 어쩔 수 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImitationResNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ImitationResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(16, 2, stride=1)\n",
    "        #self.layer2 = self._make_layer(32, 2, stride=2)\n",
    "        #self.layer3 = self._make_layer(48, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(64, 2, stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "\n",
    "    def _make_layer(self, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(BasicBlock(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.layer1(out)\n",
    "        #out = self.layer2(out)\n",
    "        #out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 14)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImitationModel = ImitationResNet().to(DEVICE)\n",
    "optimizer = optim.SGD(ImitationModel.parameters(), lr=0.1,\n",
    "                      momentum=0.9, weight_decay=0.0005)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    bar_total = tqdm_notebook(enumerate(train_loader), total=len(train_loader))\n",
    "    for batch_idx, (data, target) in bar_total:\n",
    "        bar_total.set_description(\"{0}번째 학습 - {1}번 배치\".format(epoch, batch_idx))\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        #print(output.data)\n",
    "        #output = output.max(1, keepdim=True)[1]\n",
    "        #target = torch.max(target, 2)[1].squeeze(1)\n",
    "        #print(output.data)\n",
    "        #print(target.data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    lastoutput = None\n",
    "    lasttarget = None\n",
    "    with torch.no_grad():\n",
    "        bar_total = tqdm_notebook(enumerate(test_loader), total=len(test_loader))\n",
    "        for batch_idx, (data, target) in bar_total:\n",
    "            bar_total.set_description(\"테스트 중 - {0}번 배치\".format(batch_idx))\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "            #target = torch.max(target, 2)[1].squeeze(1)\n",
    "            lastoutput = output\n",
    "            lasttarget = target\n",
    "            # 배치 오차를 합산\n",
    "            \n",
    "            loss = F.cross_entropy(output.float(), target.long(), reduction='sum')\n",
    "            test_loss += loss.item()\n",
    "            # 가장 높은 값을 가진 인덱스가 바로 예측값\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy, lastoutput.data, pred.data, lasttarget.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(path, epoch, model, optimizer, scheduler):\n",
    "    state = {\n",
    "        'Epoch': epoch,\n",
    "        'State_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'scheduler': scheduler.state_dict()\n",
    "    }\n",
    "    torch.save(state, path)\n",
    "    \n",
    "def load_checkpoint(path, model, optimitzer, scheduler):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['State_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "    epoch = int(checkpoint['Epoch'])\n",
    "    return model, optimizer, epoch\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (7) FGSM 이미지 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(image, epsilon, gradient):\n",
    "    sign_gradient = gradient.sign()\n",
    "    perturbed_image = image + epsilon * sign_gradient\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    return perturbed_image\n",
    "\n",
    "def getPertubedImage(img_tensor, target, epsilon, model):\n",
    "    img_tensor = img_tensor#.unsqueeze(0)\n",
    "    img_tensor.requires_grad_(True)\n",
    "    output = model(img_tensor)\n",
    "    loss = F.nll_loss(output, target) \n",
    "    \n",
    "    model.zero_grad()\n",
    "    \n",
    "    loss.requires_grad_(True)\n",
    "    loss.backward()\n",
    "    \n",
    "    gradient = img_tensor.grad.data\n",
    "    perturbed_data = fgsm_attack(img_tensor, epsilon, gradient)\n",
    "    return perturbed_data#.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = (img+1)/2    \n",
    "    img = img.squeeze()\n",
    "    np_img = img.detach().numpy()\n",
    "    plt.imshow(np_img, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "공격 성공 여부 확인할 만한 데이터셋의 조건 \n",
    "\n",
    " > (1) 정상 이미지에서 실제 답안 == 원본 모델 답안 == 모방 모델 답안\n",
    "\n",
    "-------------------------------\n",
    "\n",
    "공격 성공 조건\n",
    "\n",
    "> (1) 정상 이미지에서 실제 답안 != 왜곡 이미지(모방)에서 원본 모델 답안\n",
    ">\n",
    "> (2) 왜곡 이미지(모방 모델로 생성)에서 원본 모델 답안 == 왜곡 이미지(원본 모델로 생성)에서 원본 모델 답안\n",
    ">\n",
    "> (3) 왜곡 이미지(모방 모델로 생성)에서 모방 모델 답안 == 왜곡 이미지(원본 모델로 생성)에서 모방 모델 답안"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_FGSM_attack(model, willBeHackedModel, dataloader, epsilon=0.07) :\n",
    "    success_num=0\n",
    "    bar_total = tqdm_notebook(enumerate(dataloader), total=len(dataloader))\n",
    "    for batch_idx, (data, target) in bar_total:       \n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        realTarget = target\n",
    "        perturbed_image = getPertubedImage(data, realTarget, epsilon, model)\n",
    "        \n",
    "\n",
    "        \n",
    "        if target.item() != willBeHackedModel(data).max(1, keepdim=True)[1].item() or target.item() != model(data).max(1, keepdim=True)[1].item() :\n",
    "            continue\n",
    "        #if willBeHackedModel(perturbed_image).max(1, keepdim=True)[1].item() != model(perturbed_image).max(1, keepdim=True)[1].item() :\n",
    "            #continue\n",
    "        #if target.item() == willBeHackedModel(perturbed_image).max(1, keepdim=True)[1].item() or target.item() == model(perturbed_image).max(1, keepdim=True)[1].item() :\n",
    "            #continue\n",
    "            \n",
    "        imitaionTarget = model(data).max(1, keepdim=True)[1].squeeze(0)\n",
    "        perturbed_original_image = getPertubedImage(data, target, epsilon, willBeHackedModel)\n",
    "        \n",
    "        #if target.item() == willBeHackedModel(perturbed_image).max(1, keepdim=True)[1].item():\n",
    "            #continue\n",
    "        \n",
    "        #if willBeHackedModel(perturbed_original_image).max(1, keepdim=False)[1].item() != willBeHackedModel(perturbed_image).max(1, keepdim=False)[1].item():\n",
    "            #continue\n",
    "        #if model(perturbed_original_image).max(1, keepdim=False)[1].item() != model(perturbed_image).max(1, keepdim=False)[1].item():\n",
    "            #continue\n",
    "            \n",
    "            \n",
    "        imshow(data[0])\n",
    "        print(\"모범 답안 : \" + str(target.item()))\n",
    "        print(\"----------------------------------------------------------------------------------------------------\")\n",
    "        print(\"원본 이미지 원본 모델 예측 : \" + str(willBeHackedModel(data).max(1, keepdim=False)[1].item()))\n",
    "        print(\"원본 이미지 모방 모델 예측 : \" + str(model(data).max(1, keepdim=False)[1].item()))\n",
    "        print(\"----------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "        realTarget = target\n",
    "        perturbed_image = getPertubedImage(data, realTarget, epsilon, model)\n",
    "        imshow(perturbed_image)\n",
    "        #pic = np.transpose(perturbed_image.detach().numpy()[0], (1, 2, 0))\n",
    "        #plt.imshow(pic)\n",
    "        #plt.show()\n",
    "        print(\"왜곡 이미지(타겟 = real) 원본 모델 예측 : \" + str(willBeHackedModel(perturbed_image).max(1, keepdim=False)[1].item()))\n",
    "        print(\"왜곡 이미지(타겟 = real) 모방 모델 예측 : \" + str(model(perturbed_image).max(1, keepdim=False)[1].item()))\n",
    "        print(\"----------------------------------------------------------------------------------------------------\")\n",
    "        originalTarget = willBeHackedModel(data).max(1, keepdim=True)[1].squeeze(0)\n",
    "        perturbed_image = getPertubedImage(data, originalTarget, epsilon, model)\n",
    "        print(\"왜곡 이미지(타겟 = 원본 모델) 원본 모델 예측 : \" + str(willBeHackedModel(perturbed_image).max(1, keepdim=False)[1].item()))\n",
    "        print(\"왜곡 이미지(타겟 = 원본 모델) 모방 모델 예측 : \" + str(model(perturbed_image).max(1, keepdim=False)[1].item()))\n",
    "        print(\"----------------------------------------------------------------------------------------------------\")\n",
    "        imitaionTarget = model(data).max(1, keepdim=True)[1].squeeze(0)\n",
    "        perturbed_image = getPertubedImage(data, imitaionTarget, epsilon, model)\n",
    "        print(\"왜곡 이미지(타겟 = 모방 모델) 원본 모델 예측 : \" + str(willBeHackedModel(perturbed_image).max(1, keepdim=False)[1].item()))\n",
    "        print(\"왜곡 이미지(타겟 = 모방 모델) 모방 모델 예측 : \" + str(model(perturbed_image).max(1, keepdim=False)[1].item()))\n",
    "        print(\"----------------------------------------------------------------------------------------------------\")\n",
    "        realTarget = target\n",
    "        perturbed_image = getPertubedImage(data, realTarget, epsilon, willBeHackedModel)\n",
    "        print(\"왜곡 이미지(모델 = 원본) 원본 모델 예측 : \" + str(willBeHackedModel(perturbed_image).max(1, keepdim=False)[1].item()))\n",
    "        print(\"왜곡 이미지(모델 = 원본) 모방 모델 예측 : \" + str(model(perturbed_image).max(1, keepdim=False)[1].item()))\n",
    "        break\n",
    "    #print(str(success_num) + \"/\" + str(len(dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_num_attack_succeess(model, willBeHackedModel, dataloader, epsilon=0.07) :\n",
    "    success_num=0\n",
    "    total_correct=0\n",
    "    bar_total = tqdm_notebook(enumerate(dataloader), total=len(dataloader))\n",
    "    for batch_idx, (data, target) in bar_total:    \n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        bar_total.set_description(\"공격 성공 : {0}/{1}\".format(success_num, total_correct))\n",
    "        realTarget = target\n",
    "        perturbed_image = getPertubedImage(data, realTarget, epsilon, model)\n",
    "        \n",
    "        #원본 이미지에서 정답을 맞췄는가. 틀렸으면 논외로 성공도 실패도 아님.\n",
    "        if target.item() != willBeHackedModel(data).max(1, keepdim=True)[1].item() or target.item() != model(data).max(1, keepdim=True)[1].item() :\n",
    "            continue\n",
    "        else :\n",
    "            total_correct += 1\n",
    "\n",
    "        #답안이 실제랑 같으면 실패\n",
    "        if target.item() == willBeHackedModel(perturbed_image).max(1, keepdim=True)[1].item():\n",
    "            continue\n",
    "        \n",
    "        #원본 모델로 FGSM 공격 할때와 같은 결과가 나오면 공격 성공\n",
    "        perturbed_original_image = getPertubedImage(data, target, epsilon, willBeHackedModel)\n",
    "        if willBeHackedModel(perturbed_original_image).max(1, keepdim=False)[1].item() != willBeHackedModel(perturbed_image).max(1, keepdim=False)[1].item():\n",
    "            continue\n",
    "        if model(perturbed_original_image).max(1, keepdim=False)[1].item() != model(perturbed_image).max(1, keepdim=False)[1].item():\n",
    "            continue\n",
    "            \n",
    "        #imshow(data[0])\n",
    "        #print(\"모범 답안 : \" + str(target.item()))\n",
    "        #print(\"----------------------------------------------------------------------------------------------------\")\n",
    "        #print(\"원본 이미지 원본 모델 예측 : \" + str(willBeHackedModel(data).max(1, keepdim=False)[1].item()))\n",
    "        #print(\"원본 이미지 모방 모델 예측 : \" + str(model(data).max(1, keepdim=False)[1].item()))\n",
    "        #print(\"----------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "        realTarget = target\n",
    "        perturbed_image = getPertubedImage(data, realTarget, epsilon, model)\n",
    "        #pic = np.transpose(perturbed_image.detach().numpy()[0], (1, 2, 0))\n",
    "        #plt.imshow(pic)\n",
    "        #plt.show()\n",
    "        #print(\"왜곡 이미지(타겟 = real) 원본 모델 예측 : \" + str(willBeHackedModel(perturbed_image).max(1, keepdim=False)[1].item()))\n",
    "        #print(\"왜곡 이미지(타겟 = real) 모방 모델 예측 : \" + str(model(perturbed_image).max(1, keepdim=False)[1].item()))\n",
    "        #print(\"----------------------------------------------------------------------------------------------------\")\n",
    "        originalTarget = willBeHackedModel(data).max(1, keepdim=True)[1].squeeze(0)\n",
    "        perturbed_image = getPertubedImage(data, originalTarget, epsilon, model)\n",
    "        \n",
    "        #print(\"왜곡 이미지(타겟 = 원본 모델) 원본 모델 예측 : \" + str(willBeHackedModel(perturbed_image).max(1, keepdim=False)[1].item()))\n",
    "        #print(\"왜곡 이미지(타겟 = 원본 모델) 모방 모델 예측 : \" + str(model(perturbed_image).max(1, keepdim=False)[1].item()))\n",
    "        #print(\"----------------------------------------------------------------------------------------------------\")\n",
    "        imitaionTarget = model(data).max(1, keepdim=True)[1].squeeze(0)\n",
    "        perturbed_image = getPertubedImage(data, imitaionTarget, epsilon, model)\n",
    "        #print(\"왜곡 이미지(타겟 = 모방 모델) 원본 모델 예측 : \" + str(willBeHackedModel(perturbed_image).max(1, keepdim=False)[1].item()))\n",
    "        #print(\"왜곡 이미지(타겟 = 모방 모델) 모방 모델 예측 : \" + str(model(perturbed_image).max(1, keepdim=False)[1].item()))\n",
    "        #print(\"----------------------------------------------------------------------------------------------------\")\n",
    "        imitaionTarget = model(data).max(1, keepdim=True)[1].squeeze(0)\n",
    "        perturbed_original_image = getPertubedImage(data, imitaionTarget, epsilon, willBeHackedModel)\n",
    "        #print(\"왜곡 이미지(모델 = 원본) 원본 모델 예측 : \" + str(willBeHackedModel(perturbed_image).max(1, keepdim=False)[1].item()))\n",
    "        #print(\"왜곡 이미지(모델 = 원본) 모방 모델 예측 : \" + str(model(perturbed_image).max(1, keepdim=False)[1].item()))\n",
    "        success_num+=1\n",
    "    print(\"공격 성공 횟수 : \" + str(success_num) + \"/\" + str(total_correct)) \n",
    "    print(\"공격 성공률 (백분율) : {0}%\".format(success_num*100/total_correct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (8) 학습 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3cd113b7b8247839c482de2ae80c22f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모범 답안 : 2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "원본 이미지 원본 모델 예측 : 2\n",
      "원본 이미지 모방 모델 예측 : 2\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "왜곡 이미지(타겟 = real) 원본 모델 예측 : 1\n",
      "왜곡 이미지(타겟 = real) 모방 모델 예측 : 8\n",
      "----------------------------------------------------------------------------------------------------\n",
      "왜곡 이미지(타겟 = 원본 모델) 원본 모델 예측 : 1\n",
      "왜곡 이미지(타겟 = 원본 모델) 모방 모델 예측 : 8\n",
      "----------------------------------------------------------------------------------------------------\n",
      "왜곡 이미지(타겟 = 모방 모델) 원본 모델 예측 : 1\n",
      "왜곡 이미지(타겟 = 모방 모델) 모방 모델 예측 : 8\n",
      "----------------------------------------------------------------------------------------------------\n",
      "왜곡 이미지(모델 = 원본) 원본 모델 예측 : 1\n",
      "왜곡 이미지(모델 = 원본) 모방 모델 예측 : 8\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[2] Test Loss: 0.1464, Accuracy: 95.60%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "646011b2c84849a485f2c64666fe8944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=469.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-158-e4a58725b0c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malready\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mImitationModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_o\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_p\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mImitationModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mclear_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-151-6e245607b7b4>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, optimizer, epoch)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;31m#print(target.data)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \"\"\"\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "already=1\n",
    "for epoch in range(already, EPOCHS + 1):\n",
    "    scheduler.step()\n",
    "    train(ImitationModel, train_loader, optimizer, epoch)\n",
    "    test_loss, test_accuracy, _o, _p, _t = evaluate(ImitationModel, test_loader)\n",
    "    clear_output(wait=True)\n",
    "    #print(\"아웃풋 : \" + str(_o))\n",
    "    #print(\"모방 모델의 답안 : \" + str(_p))\n",
    "    #print(\"원본 모델의 답안 : \" + str(_t))\n",
    "    test_FGSM_attack(ImitationModel, willBeHackedModel, test_cifar_loader)\n",
    "    print(\"----------------------------------------------------------------------------------------------------\")\n",
    "    print('[{}] Test Loss: {:.4f}, Accuracy: {:.2f}%'.format(\n",
    "          epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LHZ\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af5785f82e324e6da14ba34dbf5b4a8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모범 답안 : 5\n",
      "----------------------------------------------------------------------------------------------------\n",
      "원본 이미지 원본 모델 예측 : 5\n",
      "원본 이미지 모방 모델 예측 : 5\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "왜곡 이미지(타겟 = real) 원본 모델 예측 : 1\n",
      "왜곡 이미지(타겟 = real) 모방 모델 예측 : 5\n",
      "----------------------------------------------------------------------------------------------------\n",
      "왜곡 이미지(타겟 = 원본 모델) 원본 모델 예측 : 1\n",
      "왜곡 이미지(타겟 = 원본 모델) 모방 모델 예측 : 5\n",
      "----------------------------------------------------------------------------------------------------\n",
      "왜곡 이미지(타겟 = 모방 모델) 원본 모델 예측 : 1\n",
      "왜곡 이미지(타겟 = 모방 모델) 모방 모델 예측 : 5\n",
      "----------------------------------------------------------------------------------------------------\n",
      "왜곡 이미지(모델 = 원본) 원본 모델 예측 : 1\n",
      "왜곡 이미지(모델 = 원본) 모방 모델 예측 : 5\n"
     ]
    }
   ],
   "source": [
    "test_FGSM_attack(ImitationModel, willBeHackedModel, test_cifar_loader, epsilon=0.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LHZ\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1233f0512101412aba822ef82312dbba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "공격 성공 횟수 : 2168/2262\n",
      "공격 성공률 (백분율) : 95.84438549955792%\n"
     ]
    }
   ],
   "source": [
    "get_num_attack_succeess(ImitationModel, willBeHackedModel, test_cifar_loader, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LHZ\\anaconda3\\lib\\site-packages\\torch\\serialization.py:402: UserWarning: Couldn't retrieve source code for container of type BasicBlock. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(ImitationModel, './model/Attack Model/model_4.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_checkpoint(\"checkpoints/model_1/checkpoints4.tar\", already, model, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LHZ\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83e00e9f5cf14ec78ee6925155f9a511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=79.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3.7169133827209473,\n",
       " 65.56,\n",
       " tensor([[ -1.3295,  -8.1340, -13.4282,  -5.3378,   5.5988,   5.3002, -11.8622,\n",
       "           43.7479,  -6.3951,  -8.1678],\n",
       "         [ -9.6390,  -5.4726,  -5.8292,   9.4391,  -3.4087,  26.4297,  -5.8986,\n",
       "           12.8656, -12.2009,  -6.2901],\n",
       "         [ -2.3526,   3.7780,  -6.7665,  -1.7601,  -8.3274,  -5.3754,  -6.2733,\n",
       "           -0.8379,   9.4810,  18.4347],\n",
       "         [ -1.9152,  14.9927,  -5.6941,   2.5607,  -8.3189,  -0.5827,   1.8324,\n",
       "           -6.6908,   6.9678,  -3.1544],\n",
       "         [  6.8326,   2.6636,   6.0283,  -3.5994,  -5.0497,  -8.0440,  -3.6280,\n",
       "           -2.1835,  13.7585,  -6.7769],\n",
       "         [-10.1441,  11.5962,  -4.4965,  -0.2949,  -1.7003,   2.7460,  -0.0768,\n",
       "           -7.5410,  -8.6675,  18.5780],\n",
       "         [-11.3807, -15.3519, -12.4055,  24.9558,   2.2705,  32.0103,   2.7885,\n",
       "            5.3101, -18.3358,  -9.8681],\n",
       "         [ 27.6686,  -5.6602,  -0.4504,   6.8777,  -0.7883,  -9.7300,  -5.5564,\n",
       "           -3.4166,  -2.9285,  -6.0172],\n",
       "         [ -8.2508,  17.9800,  -0.9673,   0.1278, -12.4506,  17.2123,  -4.8864,\n",
       "           -7.8817,  -9.7573,   8.8712],\n",
       "         [-17.4021, -20.4770,  -9.8746,  27.9118,   1.0169,  56.5916, -11.5248,\n",
       "            9.6808, -21.6649, -14.2696],\n",
       "         [  2.2716,  -2.5667,  -2.0095,   6.0550,  -1.9338,   1.5492,  -4.0636,\n",
       "            7.4601,  -2.8999,  -3.8644],\n",
       "         [  2.2563,   2.7499,  -3.0485,  10.4410,   3.7708,  -1.1624,  -4.8525,\n",
       "           -6.7647,  -1.4267,  -1.9647],\n",
       "         [-13.7299, -18.3445, -15.0210,   9.4089,   8.1491,  36.2589, -11.1201,\n",
       "           30.3268, -16.4524,  -9.4865],\n",
       "         [ -9.1178,  -9.6342,  -2.0514,   7.5296,   9.3743,   7.1563,  -1.4103,\n",
       "           15.5265,  -9.0789,  -8.3000],\n",
       "         [ -2.5018,  -4.3710,  -2.6966,   6.0608,   4.5541,   2.6295,   4.5244,\n",
       "            1.4031,  -4.7426,  -4.8627],\n",
       "         [ -1.3071,  30.9449,  -3.9548,   0.2105, -10.8506,   5.2409,  -5.1687,\n",
       "          -10.3592,  -1.2272,  -3.5337]]),\n",
       " tensor([[7],\n",
       "         [5],\n",
       "         [9],\n",
       "         [1],\n",
       "         [8],\n",
       "         [9],\n",
       "         [5],\n",
       "         [0],\n",
       "         [1],\n",
       "         [5],\n",
       "         [7],\n",
       "         [3],\n",
       "         [5],\n",
       "         [7],\n",
       "         [3],\n",
       "         [1]]),\n",
       " tensor([7, 5, 9, 1, 8, 9, 6, 0, 9, 5, 3, 9, 7, 5, 6, 1]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_m, _o, _e = load_checkpoint(\"checkpoints/model_1/checkpoints1.tar\", ImitationResNet(), optim.SGD(model.parameters(), lr=0.1,\n",
    "                      momentum=0.9, weight_decay=0.0005), optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
