{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Adversarial Attack (in Black-Box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "\n",
    "import random\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 사용 기기 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) 하이퍼 파라미터 셋팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS     = 300\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "MODEL_PATH = './model/PreTrained ResNet/preTrainedResNet_mnist.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) 공격할 모델 가져오기\n",
    "---------------------------------------------------------------------------\n",
    "CIFAR-10으로 미리 학습된 모델을 가져올 것이다.\n",
    "\n",
    "물론, 이 모델 그대로 FGSM 공격 이미지를 생성할 수는 있지만,\n",
    "BlackBox인 상황이라고 가정하고 진행할 것이기에,\n",
    "\n",
    "모방 모델을 만들어 FGSM 공격 이미지를 생성할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(16, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(32, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(64, 2, stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "\n",
    "    def _make_layer(self, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(BasicBlock(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, 7)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "willBeHackedModel = torch.load(MODEL_PATH)\n",
    "willBeHackedModel.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) 데이터셋 가져오기\n",
    "------------------------------------------------------------\n",
    "모방 모델 또한 CIFAR-10으로 학습할 예정이다. 같은 라벨 클래스를 가진 데이터셋이 존재하지 않아 같은 것으로 학습하겠다.\n",
    "어차피 같은 것이든 아니든, 본 CIFAR-10의 y값은 쓰지 않을 것이고, 기존 모델의 분류값을 y값으로 사용할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cifar_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('E:\\Dataset\\mnist',\n",
    "                   train=True,\n",
    "                   download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "    batch_size=1, shuffle=True)\n",
    "test_cifar_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('E:\\Dataset\\mnist',\n",
    "                   train=False, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "    batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (data, target) in enumerate(train_cifar_loader):\n",
    "    print(target.data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset): \n",
    "  def __init__(self, cifar, _model):\n",
    "    self.x_data = []\n",
    "    self.y_data = None\n",
    "    self.mix = {}\n",
    "    self.model = _model\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(cifar):\n",
    "        self.x_data.append(data)\n",
    "    random.shuffle(self.x_data)\n",
    "\n",
    "  def __len__(self): \n",
    "    return len(self.x_data)\n",
    "\n",
    "  def __getitem__(self, idx): \n",
    "    x = self.x_data[idx]\n",
    "    y = self.model(x).max(1, keepdim=False)[1].item()\n",
    "    return x.squeeze(0), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(CustomDataset(train_cifar_loader, willBeHackedModel), batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(CustomDataset(test_cifar_loader, willBeHackedModel), batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6) 모방 모델 만들기\n",
    "------------------------------------------------------------\n",
    "모방 모델은 ResNet으로 만들 것이다.\n",
    "어차피 실제로 할 때도 머신러닝 지식에 의해 개발자가 직접 선정한다고 하니...\n",
    "\n",
    "실제로 할 때는 모델을 정확하게 맞출 수 없음으로, 약간 다른 점을 주기 위해서, BasicBlock를 한 층 더 늘렸다. 이게 성능의 어떤 영향을 주든 BlackBox를 위한 코드니 차별점을 주기 위해 어쩔 수 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImitationResNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ImitationResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(16, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(32, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(48, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(64, 2, stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "\n",
    "    def _make_layer(self, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(BasicBlock(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImitationResNet().to(DEVICE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1,\n",
    "                      momentum=0.9, weight_decay=0.0005)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    bar_total = tqdm_notebook(enumerate(train_loader), total=len(train_loader))\n",
    "    for batch_idx, (data, target) in bar_total:\n",
    "        bar_total.set_description(\"{0}번째 학습 - {1}번 배치\".format(epoch, batch_idx))\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        #print(output.data)\n",
    "        #output = output.max(1, keepdim=True)[1]\n",
    "        #target = torch.max(target, 2)[1].squeeze(1)\n",
    "        #print(output.data)\n",
    "        #print(target.data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    lastoutput = None\n",
    "    lasttarget = None\n",
    "    with torch.no_grad():\n",
    "        bar_total = tqdm_notebook(enumerate(test_loader), total=len(test_loader))\n",
    "        for batch_idx, (data, target) in bar_total:\n",
    "            bar_total.set_description(\"테스트 중 - {0}번 배치\".format(batch_idx))\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "            #target = torch.max(target, 2)[1].squeeze(1)\n",
    "            lastoutput = output\n",
    "            lasttarget = target\n",
    "            # 배치 오차를 합산\n",
    "            \n",
    "            loss = F.cross_entropy(output.float(), target.long(), reduction='sum')\n",
    "            test_loss += loss.item()\n",
    "            # 가장 높은 값을 가진 인덱스가 바로 예측값\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy, lastoutput.data, pred.data, lasttarget.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(path, epoch, model, optimizer, scheduler):\n",
    "    state = {\n",
    "        'Epoch': epoch,\n",
    "        'State_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'scheduler': scheduler.state_dict()\n",
    "    }\n",
    "    torch.save(state, path)\n",
    "    \n",
    "def load_checkpoint(path, model, optimitzer, scheduler):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['State_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "    epoch = int(checkpoint['Epoch'])\n",
    "    return model, optimizer, epoch\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (7) FGSM 이미지 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(image, epsilon, gradient):\n",
    "    sign_gradient = gradient.sign()\n",
    "    perturbed_image = image + epsilon * sign_gradient\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    return perturbed_image\n",
    "\n",
    "def getPertubedImage(img_tensor, target, epsilon, model):\n",
    "    img_tensor = img_tensor#.unsqueeze(0)\n",
    "    img_tensor.requires_grad_(True)\n",
    "    output = model(img_tensor)\n",
    "    loss = F.nll_loss(output, target) \n",
    "    \n",
    "    model.zero_grad()\n",
    "    \n",
    "    loss.requires_grad_(True)\n",
    "    loss.backward()\n",
    "    \n",
    "    gradient = img_tensor.grad.data\n",
    "    perturbed_data = fgsm_attack(img_tensor, epsilon, gradient)\n",
    "    return perturbed_data#.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = (img+1)/2    \n",
    "    img = img.squeeze()\n",
    "    np_img = img.detach().numpy()\n",
    "    plt.imshow(np_img, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_FGSM_attack(model, willBeHackedModel, dataloader, epsilon=0.07) :\n",
    "    success_num=0\n",
    "    bar_total = tqdm_notebook(enumerate(dataloader), total=len(dataloader))\n",
    "    for batch_idx, (data, target) in bar_total:            \n",
    "        realTarget = target\n",
    "        perturbed_image = getPertubedImage(data, realTarget, epsilon, model)\n",
    "        #if willBeHackedModel(perturbed_image).max(1, keepdim=True)[1].item() != model(perturbed_image).max(1, keepdim=True)[1].item() :\n",
    "            #continue\n",
    "        #if target.item() == willBeHackedModel(perturbed_image).max(1, keepdim=True)[1].item() or target.item() == model(perturbed_image).max(1, keepdim=True)[1].item() :\n",
    "            #continue\n",
    "        imshow(data[0])\n",
    "        print(\"모범 답안 : \" + str(target.item()))\n",
    "        print(\"----------------------------------------------------------------------------------------------------\")\n",
    "        print(\"원본 이미지 원본 모델 예측 : \" + str(willBeHackedModel(data).max(1, keepdim=False)[1].item()))\n",
    "        print(\"원본 이미지 모방 모델 예측 : \" + str(model(data).max(1, keepdim=False)[1].item()))\n",
    "        print(\"----------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "        realTarget = target\n",
    "        perturbed_image = getPertubedImage(data, realTarget, epsilon, model)\n",
    "        #pic = np.transpose(perturbed_image.detach().numpy()[0], (1, 2, 0))\n",
    "        #plt.imshow(pic)\n",
    "        #plt.show()\n",
    "        print(\"왜곡 이미지(타겟 = real) 원본 모델 예측 : \" + str(willBeHackedModel(perturbed_image).max(1, keepdim=False)[1].item()))\n",
    "        print(\"왜곡 이미지(타겟 = real) 모방 모델 예측 : \" + str(model(perturbed_image).max(1, keepdim=False)[1].item()))\n",
    "        print(\"----------------------------------------------------------------------------------------------------\")\n",
    "        originalTarget = willBeHackedModel(data).max(1, keepdim=True)[1].squeeze(0)\n",
    "        perturbed_image = getPertubedImage(data, originalTarget, epsilon, model)\n",
    "        print(\"왜곡 이미지(타겟 = 원본 모델) 원본 모델 예측 : \" + str(willBeHackedModel(perturbed_image).max(1, keepdim=False)[1].item()))\n",
    "        print(\"왜곡 이미지(타겟 = 원본 모델) 모방 모델 예측 : \" + str(model(perturbed_image).max(1, keepdim=False)[1].item()))\n",
    "        print(\"----------------------------------------------------------------------------------------------------\")\n",
    "        imitaionTarget = model(data).max(1, keepdim=True)[1].squeeze(0)\n",
    "        perturbed_image = getPertubedImage(data, imitaionTarget, epsilon, model)\n",
    "        print(\"왜곡 이미지(타겟 = 모방 모델) 원본 모델 예측 : \" + str(willBeHackedModel(perturbed_image).max(1, keepdim=False)[1].item()))\n",
    "        print(\"왜곡 이미지(타겟 = 모방 모델) 모방 모델 예측 : \" + str(model(perturbed_image).max(1, keepdim=False)[1].item()))\n",
    "        print(\"----------------------------------------------------------------------------------------------------\")\n",
    "        imitaionTarget = model(data).max(1, keepdim=True)[1].squeeze(0)\n",
    "        perturbed_image = getPertubedImage(data, imitaionTarget, epsilon, willBeHackedModel)\n",
    "        print(\"왜곡 이미지(모델 = 원본) 원본 모델 예측 : \" + str(willBeHackedModel(perturbed_image).max(1, keepdim=False)[1].item()))\n",
    "        print(\"왜곡 이미지(모델 = 원본) 모방 모델 예측 : \" + str(model(perturbed_image).max(1, keepdim=False)[1].item()))\n",
    "        break\n",
    "    #print(str(success_num) + \"/\" + str(len(dataloader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (8) 학습 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f8c87d981e4a598ce0b6253c0ac4d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모범 답안 : 6\n",
      "----------------------------------------------------------------------------------------------------\n",
      "원본 이미지 원본 모델 예측 : 6\n",
      "원본 이미지 모방 모델 예측 : 6\n",
      "----------------------------------------------------------------------------------------------------\n",
      "왜곡 이미지(타겟 = real) 원본 모델 예측 : 1\n",
      "왜곡 이미지(타겟 = real) 모방 모델 예측 : 1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "왜곡 이미지(타겟 = 원본 모델) 원본 모델 예측 : 1\n",
      "왜곡 이미지(타겟 = 원본 모델) 모방 모델 예측 : 1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "왜곡 이미지(타겟 = 모방 모델) 원본 모델 예측 : 1\n",
      "왜곡 이미지(타겟 = 모방 모델) 모방 모델 예측 : 1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "왜곡 이미지(모델 = 원본) 원본 모델 예측 : 1\n",
      "왜곡 이미지(모델 = 원본) 모방 모델 예측 : 1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[2] Test Loss: 0.1509, Accuracy: 95.25%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48fe0a0b51044c1697007f23fbfc0bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=469.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "already=2\n",
    "for epoch in range(already, EPOCHS + 1):\n",
    "    scheduler.step()\n",
    "    train(model, train_loader, optimizer, epoch)\n",
    "    test_loss, test_accuracy, _o, _p, _t = evaluate(model, test_loader)\n",
    "    clear_output(wait=True)\n",
    "    #print(\"아웃풋 : \" + str(_o))\n",
    "    #print(\"모방 모델의 답안 : \" + str(_p))\n",
    "    #print(\"원본 모델의 답안 : \" + str(_t))\n",
    "    test_FGSM_attack(model, willBeHackedModel, test_cifar_loader)\n",
    "    print(\"----------------------------------------------------------------------------------------------------\")\n",
    "    print('[{}] Test Loss: {:.4f}, Accuracy: {:.2f}%'.format(\n",
    "          epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LHZ\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88b36e94a9834b019f74e8e8206f5533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=79.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-7f0d056973a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_FGSM_attack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwillBeHackedModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-71-4988c0e95ce1>\u001b[0m in \u001b[0;36mtest_FGSM_attack\u001b[1;34m(model, willBeHackedModel, dataloader, epsilon)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbar_total\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mrealTarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mperturbed_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetPertubedImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealTarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[1;31m#if willBeHackedModel(perturbed_image).max(1, keepdim=True)[1].item() != model(perturbed_image).max(1, keepdim=True)[1].item() :\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[1;31m#continue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-46-1aabd972b6a9>\u001b[0m in \u001b[0;36mgetPertubedImage\u001b[1;34m(img_tensor, target, epsilon, model)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mimg_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2113\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[0;32m   2114\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2115\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2116\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2117\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "test_FGSM_attack(model, willBeHackedModel, test_loader, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LHZ\\anaconda3\\lib\\site-packages\\torch\\serialization.py:402: UserWarning: Couldn't retrieve source code for container of type ImitationResNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\LHZ\\anaconda3\\lib\\site-packages\\torch\\serialization.py:402: UserWarning: Couldn't retrieve source code for container of type BasicBlock. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, './model/Attack Model/model_3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_checkpoint(\"checkpoints/model_1/checkpoints3.tar\", already, model, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LHZ\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83e00e9f5cf14ec78ee6925155f9a511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=79.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3.7169133827209473,\n",
       " 65.56,\n",
       " tensor([[ -1.3295,  -8.1340, -13.4282,  -5.3378,   5.5988,   5.3002, -11.8622,\n",
       "           43.7479,  -6.3951,  -8.1678],\n",
       "         [ -9.6390,  -5.4726,  -5.8292,   9.4391,  -3.4087,  26.4297,  -5.8986,\n",
       "           12.8656, -12.2009,  -6.2901],\n",
       "         [ -2.3526,   3.7780,  -6.7665,  -1.7601,  -8.3274,  -5.3754,  -6.2733,\n",
       "           -0.8379,   9.4810,  18.4347],\n",
       "         [ -1.9152,  14.9927,  -5.6941,   2.5607,  -8.3189,  -0.5827,   1.8324,\n",
       "           -6.6908,   6.9678,  -3.1544],\n",
       "         [  6.8326,   2.6636,   6.0283,  -3.5994,  -5.0497,  -8.0440,  -3.6280,\n",
       "           -2.1835,  13.7585,  -6.7769],\n",
       "         [-10.1441,  11.5962,  -4.4965,  -0.2949,  -1.7003,   2.7460,  -0.0768,\n",
       "           -7.5410,  -8.6675,  18.5780],\n",
       "         [-11.3807, -15.3519, -12.4055,  24.9558,   2.2705,  32.0103,   2.7885,\n",
       "            5.3101, -18.3358,  -9.8681],\n",
       "         [ 27.6686,  -5.6602,  -0.4504,   6.8777,  -0.7883,  -9.7300,  -5.5564,\n",
       "           -3.4166,  -2.9285,  -6.0172],\n",
       "         [ -8.2508,  17.9800,  -0.9673,   0.1278, -12.4506,  17.2123,  -4.8864,\n",
       "           -7.8817,  -9.7573,   8.8712],\n",
       "         [-17.4021, -20.4770,  -9.8746,  27.9118,   1.0169,  56.5916, -11.5248,\n",
       "            9.6808, -21.6649, -14.2696],\n",
       "         [  2.2716,  -2.5667,  -2.0095,   6.0550,  -1.9338,   1.5492,  -4.0636,\n",
       "            7.4601,  -2.8999,  -3.8644],\n",
       "         [  2.2563,   2.7499,  -3.0485,  10.4410,   3.7708,  -1.1624,  -4.8525,\n",
       "           -6.7647,  -1.4267,  -1.9647],\n",
       "         [-13.7299, -18.3445, -15.0210,   9.4089,   8.1491,  36.2589, -11.1201,\n",
       "           30.3268, -16.4524,  -9.4865],\n",
       "         [ -9.1178,  -9.6342,  -2.0514,   7.5296,   9.3743,   7.1563,  -1.4103,\n",
       "           15.5265,  -9.0789,  -8.3000],\n",
       "         [ -2.5018,  -4.3710,  -2.6966,   6.0608,   4.5541,   2.6295,   4.5244,\n",
       "            1.4031,  -4.7426,  -4.8627],\n",
       "         [ -1.3071,  30.9449,  -3.9548,   0.2105, -10.8506,   5.2409,  -5.1687,\n",
       "          -10.3592,  -1.2272,  -3.5337]]),\n",
       " tensor([[7],\n",
       "         [5],\n",
       "         [9],\n",
       "         [1],\n",
       "         [8],\n",
       "         [9],\n",
       "         [5],\n",
       "         [0],\n",
       "         [1],\n",
       "         [5],\n",
       "         [7],\n",
       "         [3],\n",
       "         [5],\n",
       "         [7],\n",
       "         [3],\n",
       "         [1]]),\n",
       " tensor([7, 5, 9, 1, 8, 9, 6, 0, 9, 5, 3, 9, 7, 5, 6, 1]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_m, _o, _e = load_checkpoint(\"checkpoints/model_1/checkpoints1.tar\", ImitationResNet(), optim.SGD(model.parameters(), lr=0.1,\n",
    "                      momentum=0.9, weight_decay=0.0005), optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
