{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Adversarial Attack (in Black-Box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "\n",
    "import random\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 사용 기기 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) 하이퍼 파라미터 셋팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS     = 300\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "MODEL_PATH = './model/PreTrained ResNet/preTrainedResNet_cifar10.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) 공격할 모델 가져오기\n",
    "---------------------------------------------------------------------------\n",
    "CIFAR-10으로 미리 학습된 모델을 가져올 것이다.\n",
    "\n",
    "물론, 이 모델 그대로 FGSM 공격 이미지를 생성할 수는 있지만,\n",
    "BlackBox인 상황이라고 가정하고 진행할 것이기에,\n",
    "\n",
    "모방 모델을 만들어 FGSM 공격 이미지를 생성할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(16, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(32, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(64, 2, stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "\n",
    "    def _make_layer(self, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(BasicBlock(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "willBeHackedModel = torch.load(MODEL_PATH)\n",
    "willBeHackedModel.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) 데이터셋 가져오기\n",
    "------------------------------------------------------------\n",
    "모방 모델 또한 CIFAR-10으로 학습할 예정이다. 같은 라벨 클래스를 가진 데이터셋이 존재하지 않아 같은 것으로 학습하겠다.\n",
    "어차피 같은 것이든 아니든, 본 CIFAR-10의 y값은 쓰지 않을 것이고, 기존 모델의 분류값을 y값으로 사용할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_cifar_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('E:\\Dataset\\CIFAR-10',\n",
    "                   train=True,\n",
    "                   download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.RandomCrop(32, padding=4),\n",
    "                       transforms.RandomHorizontalFlip(),\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                            (0.5, 0.5, 0.5))])),\n",
    "    batch_size=1, shuffle=True)\n",
    "test_cifar_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('E:\\Dataset\\CIFAR-10',\n",
    "                   train=False, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                            (0.5, 0.5, 0.5))])),\n",
    "    batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (data, target) in enumerate(train_cifar_loader):\n",
    "    print(target.data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset): \n",
    "  def __init__(self, cifar, _model):\n",
    "    self.x_data = []\n",
    "    self.y_data = None\n",
    "    self.mix = {}\n",
    "    self.model = _model\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(cifar):\n",
    "        self.x_data.append(data)\n",
    "    random.shuffle(self.x_data)\n",
    "\n",
    "  def __len__(self): \n",
    "    return len(self.x_data)\n",
    "\n",
    "  def __getitem__(self, idx): \n",
    "    x = self.x_data[idx]\n",
    "    y = self.model(x)\n",
    "    return x.squeeze(0), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(CustomDataset(train_cifar_loader, willBeHackedModel), batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(CustomDataset(test_cifar_loader, willBeHackedModel), batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6) 모방 모델 만들기\n",
    "------------------------------------------------------------\n",
    "모방 모델은 ResNet으로 만들 것이다.\n",
    "어차피 실제로 할 때도 머신러닝 지식에 의해 개발자가 직접 선정한다고 하니...\n",
    "\n",
    "실제로 할 때는 모델을 정확하게 맞출 수 없음으로, 약간 다른 점을 주기 위해서, BasicBlock를 한 층 더 늘렸다. 이게 성능의 어떤 영향을 주든 BlackBox를 위한 코드니 차별점을 주기 위해 어쩔 수 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImitationResNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ImitationResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(16, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(32, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(64, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(128, 2, stride=2)\n",
    "        self.linear = nn.Linear(128, num_classes)\n",
    "\n",
    "    def _make_layer(self, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(BasicBlock(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImitationResNet().to(DEVICE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1,\n",
    "                      momentum=0.9, weight_decay=0.0005)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        print(\"{0}번째 학습 - 배치 : {1}\".format(epoch, batch_idx))\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        #print(output.data)\n",
    "        #output = output.max(1, keepdim=True)[1]\n",
    "        target = torch.max(target, 2)[1].squeeze(1)\n",
    "        #print(output.data)\n",
    "        #print(target.data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    lastoutput = None\n",
    "    lasttarget = None\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "            target = torch.max(target, 2)[1].squeeze(1)\n",
    "            lastoutput = output\n",
    "            lasttarget = target\n",
    "            # 배치 오차를 합산\n",
    "            \n",
    "            loss = F.cross_entropy(output.float(), target.long(), reduction='sum')\n",
    "            test_loss += loss.item()\n",
    "            # 가장 높은 값을 가진 인덱스가 바로 예측값\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy, lastoutput.data, pred.data, lasttarget.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아웃풋 : tensor([[-3.2847e+00, -1.6174e+00,  3.7283e+00,  1.4187e+00,  8.5795e-01,\n",
      "         -1.6557e+00,  5.6282e+00, -7.1910e-01, -2.4095e+00, -2.0990e+00],\n",
      "        [ 2.6056e+00, -1.2933e+00, -1.2510e+00, -3.9994e+00, -3.4186e-01,\n",
      "         -2.7036e+00, -6.0008e+00,  7.1966e+00,  2.3821e+00,  3.3127e+00],\n",
      "        [-1.3029e+00, -3.4729e+00,  3.4347e+00, -2.8521e+00,  5.3294e+00,\n",
      "          4.9592e-01, -2.5589e+00,  7.3172e+00, -3.3434e+00, -3.3056e+00],\n",
      "        [-1.8058e+00,  1.9481e+00, -4.3818e+00, -1.8813e+00, -2.8436e+00,\n",
      "         -1.5563e+00, -2.7347e+00, -2.6572e-01,  5.7401e+00,  7.7219e+00],\n",
      "        [-5.6945e+00, -3.5585e-01, -1.4526e+00,  3.7056e+00, -3.1023e+00,\n",
      "          7.3364e+00, -1.3573e+00,  2.6553e+00, -3.1113e+00,  1.2747e+00],\n",
      "        [-2.1099e+00, -5.2222e+00,  1.2237e+01,  1.7992e+00, -2.5531e-01,\n",
      "         -1.4426e+00,  3.6295e-01, -1.0739e+00, -8.1895e-01, -3.5424e+00],\n",
      "        [-5.3807e+00, -3.0027e+00,  2.4780e-01,  5.9044e+00, -2.1717e+00,\n",
      "          2.8770e+00, -3.9131e+00,  3.7401e+00, -2.8416e-01,  1.9014e+00],\n",
      "        [ 1.0251e+00, -3.3854e+00, -1.2887e-01, -3.9195e+00, -2.5685e+00,\n",
      "         -4.2401e-01, -6.6684e+00,  1.2620e+01,  2.3944e+00,  9.3020e-01],\n",
      "        [-2.7614e+00, -1.9566e+00,  5.7677e+00, -7.8606e-01,  1.3413e+00,\n",
      "          3.4743e+00,  8.9622e-01, -6.8401e-01, -2.3553e+00, -3.0375e+00],\n",
      "        [ 7.3305e-01,  1.3620e+01, -2.7407e+00, -3.7407e+00, -2.6680e+00,\n",
      "         -5.6419e+00, -4.9081e-02, -2.9899e+00,  1.3186e+00,  2.0780e+00],\n",
      "        [-3.6146e-01,  5.4800e-01,  1.2746e+00,  4.2118e-01, -4.6297e-02,\n",
      "         -2.8473e+00,  9.7557e-01, -1.1333e+00,  1.0886e+00,  1.2331e-02],\n",
      "        [ 3.2475e+00,  1.2283e+00, -1.9694e+00, -1.0197e+00, -2.1173e+00,\n",
      "         -4.0407e+00, -2.5800e+00, -9.3391e-01,  7.1814e+00,  1.0500e+00],\n",
      "        [ 9.8041e+00, -1.6613e+00,  2.9183e-01, -3.1918e+00, -1.8777e+00,\n",
      "         -6.8368e+00, -4.3698e+00,  3.6574e+00,  3.5527e+00,  6.3587e-01],\n",
      "        [-2.3394e+00,  7.0934e-01, -2.5786e+00,  1.1318e+00, -2.5355e+00,\n",
      "         -1.1355e+00,  1.4346e+00,  9.9431e-01,  2.2983e+00,  1.9464e+00],\n",
      "        [ 1.9514e+00,  1.4642e-01,  2.4651e+00,  3.2267e-01, -3.3155e+00,\n",
      "          1.7607e-01, -1.6744e+00, -1.1746e+00,  1.8679e+00, -7.2453e-01],\n",
      "        [-3.6600e+00, -2.0195e+00,  1.2223e+00,  2.6422e+00, -2.8718e+00,\n",
      "          8.0940e+00, -3.6080e+00,  3.2484e+00, -2.4692e+00, -6.6193e-01]])\n",
      "모방 모델의 답안 : tensor([[6],\n",
      "        [7],\n",
      "        [7],\n",
      "        [9],\n",
      "        [5],\n",
      "        [2],\n",
      "        [3],\n",
      "        [7],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [8],\n",
      "        [0],\n",
      "        [8],\n",
      "        [2],\n",
      "        [5]])\n",
      "원본 모델의 답안 : tensor([6, 9, 7, 9, 5, 2, 3, 7, 5, 1, 3, 8, 0, 6, 0, 5])\n",
      "[10] Test Loss: 0.7096, Accuracy: 75.79%\n",
      "11번째 학습 - 배치 : 0\n",
      "11번째 학습 - 배치 : 1\n",
      "11번째 학습 - 배치 : 2\n",
      "11번째 학습 - 배치 : 3\n",
      "11번째 학습 - 배치 : 4\n",
      "11번째 학습 - 배치 : 5\n",
      "11번째 학습 - 배치 : 6\n",
      "11번째 학습 - 배치 : 7\n",
      "11번째 학습 - 배치 : 8\n",
      "11번째 학습 - 배치 : 9\n",
      "11번째 학습 - 배치 : 10\n",
      "11번째 학습 - 배치 : 11\n",
      "11번째 학습 - 배치 : 12\n",
      "11번째 학습 - 배치 : 13\n",
      "11번째 학습 - 배치 : 14\n",
      "11번째 학습 - 배치 : 15\n",
      "11번째 학습 - 배치 : 16\n",
      "11번째 학습 - 배치 : 17\n",
      "11번째 학습 - 배치 : 18\n",
      "11번째 학습 - 배치 : 19\n",
      "11번째 학습 - 배치 : 20\n",
      "11번째 학습 - 배치 : 21\n",
      "11번째 학습 - 배치 : 22\n",
      "11번째 학습 - 배치 : 23\n",
      "11번째 학습 - 배치 : 24\n",
      "11번째 학습 - 배치 : 25\n",
      "11번째 학습 - 배치 : 26\n",
      "11번째 학습 - 배치 : 27\n",
      "11번째 학습 - 배치 : 28\n",
      "11번째 학습 - 배치 : 29\n",
      "11번째 학습 - 배치 : 30\n",
      "11번째 학습 - 배치 : 31\n",
      "11번째 학습 - 배치 : 32\n",
      "11번째 학습 - 배치 : 33\n",
      "11번째 학습 - 배치 : 34\n",
      "11번째 학습 - 배치 : 35\n",
      "11번째 학습 - 배치 : 36\n",
      "11번째 학습 - 배치 : 37\n",
      "11번째 학습 - 배치 : 38\n",
      "11번째 학습 - 배치 : 39\n",
      "11번째 학습 - 배치 : 40\n",
      "11번째 학습 - 배치 : 41\n",
      "11번째 학습 - 배치 : 42\n",
      "11번째 학습 - 배치 : 43\n",
      "11번째 학습 - 배치 : 44\n",
      "11번째 학습 - 배치 : 45\n",
      "11번째 학습 - 배치 : 46\n",
      "11번째 학습 - 배치 : 47\n",
      "11번째 학습 - 배치 : 48\n",
      "11번째 학습 - 배치 : 49\n",
      "11번째 학습 - 배치 : 50\n",
      "11번째 학습 - 배치 : 51\n",
      "11번째 학습 - 배치 : 52\n",
      "11번째 학습 - 배치 : 53\n",
      "11번째 학습 - 배치 : 54\n",
      "11번째 학습 - 배치 : 55\n",
      "11번째 학습 - 배치 : 56\n",
      "11번째 학습 - 배치 : 57\n",
      "11번째 학습 - 배치 : 58\n",
      "11번째 학습 - 배치 : 59\n",
      "11번째 학습 - 배치 : 60\n",
      "11번째 학습 - 배치 : 61\n",
      "11번째 학습 - 배치 : 62\n",
      "11번째 학습 - 배치 : 63\n",
      "11번째 학습 - 배치 : 64\n",
      "11번째 학습 - 배치 : 65\n",
      "11번째 학습 - 배치 : 66\n",
      "11번째 학습 - 배치 : 67\n",
      "11번째 학습 - 배치 : 68\n",
      "11번째 학습 - 배치 : 69\n",
      "11번째 학습 - 배치 : 70\n",
      "11번째 학습 - 배치 : 71\n",
      "11번째 학습 - 배치 : 72\n",
      "11번째 학습 - 배치 : 73\n",
      "11번째 학습 - 배치 : 74\n",
      "11번째 학습 - 배치 : 75\n",
      "11번째 학습 - 배치 : 76\n",
      "11번째 학습 - 배치 : 77\n",
      "11번째 학습 - 배치 : 78\n",
      "11번째 학습 - 배치 : 79\n",
      "11번째 학습 - 배치 : 80\n",
      "11번째 학습 - 배치 : 81\n",
      "11번째 학습 - 배치 : 82\n",
      "11번째 학습 - 배치 : 83\n",
      "11번째 학습 - 배치 : 84\n",
      "11번째 학습 - 배치 : 85\n",
      "11번째 학습 - 배치 : 86\n",
      "11번째 학습 - 배치 : 87\n",
      "11번째 학습 - 배치 : 88\n",
      "11번째 학습 - 배치 : 89\n",
      "11번째 학습 - 배치 : 90\n",
      "11번째 학습 - 배치 : 91\n",
      "11번째 학습 - 배치 : 92\n",
      "11번째 학습 - 배치 : 93\n",
      "11번째 학습 - 배치 : 94\n",
      "11번째 학습 - 배치 : 95\n",
      "11번째 학습 - 배치 : 96\n",
      "11번째 학습 - 배치 : 97\n",
      "11번째 학습 - 배치 : 98\n",
      "11번째 학습 - 배치 : 99\n",
      "11번째 학습 - 배치 : 100\n",
      "11번째 학습 - 배치 : 101\n",
      "11번째 학습 - 배치 : 102\n",
      "11번째 학습 - 배치 : 103\n",
      "11번째 학습 - 배치 : 104\n",
      "11번째 학습 - 배치 : 105\n",
      "11번째 학습 - 배치 : 106\n",
      "11번째 학습 - 배치 : 107\n",
      "11번째 학습 - 배치 : 108\n",
      "11번째 학습 - 배치 : 109\n",
      "11번째 학습 - 배치 : 110\n",
      "11번째 학습 - 배치 : 111\n",
      "11번째 학습 - 배치 : 112\n",
      "11번째 학습 - 배치 : 113\n",
      "11번째 학습 - 배치 : 114\n",
      "11번째 학습 - 배치 : 115\n",
      "11번째 학습 - 배치 : 116\n",
      "11번째 학습 - 배치 : 117\n",
      "11번째 학습 - 배치 : 118\n",
      "11번째 학습 - 배치 : 119\n",
      "11번째 학습 - 배치 : 120\n",
      "11번째 학습 - 배치 : 121\n",
      "11번째 학습 - 배치 : 122\n",
      "11번째 학습 - 배치 : 123\n",
      "11번째 학습 - 배치 : 124\n",
      "11번째 학습 - 배치 : 125\n",
      "11번째 학습 - 배치 : 126\n",
      "11번째 학습 - 배치 : 127\n",
      "11번째 학습 - 배치 : 128\n",
      "11번째 학습 - 배치 : 129\n",
      "11번째 학습 - 배치 : 130\n",
      "11번째 학습 - 배치 : 131\n",
      "11번째 학습 - 배치 : 132\n",
      "11번째 학습 - 배치 : 133\n",
      "11번째 학습 - 배치 : 134\n",
      "11번째 학습 - 배치 : 135\n",
      "11번째 학습 - 배치 : 136\n",
      "11번째 학습 - 배치 : 137\n",
      "11번째 학습 - 배치 : 138\n",
      "11번째 학습 - 배치 : 139\n",
      "11번째 학습 - 배치 : 140\n",
      "11번째 학습 - 배치 : 141\n",
      "11번째 학습 - 배치 : 142\n",
      "11번째 학습 - 배치 : 143\n",
      "11번째 학습 - 배치 : 144\n",
      "11번째 학습 - 배치 : 145\n",
      "11번째 학습 - 배치 : 146\n",
      "11번째 학습 - 배치 : 147\n",
      "11번째 학습 - 배치 : 148\n",
      "11번째 학습 - 배치 : 149\n",
      "11번째 학습 - 배치 : 150\n",
      "11번째 학습 - 배치 : 151\n",
      "11번째 학습 - 배치 : 152\n",
      "11번째 학습 - 배치 : 153\n",
      "11번째 학습 - 배치 : 154\n",
      "11번째 학습 - 배치 : 155\n",
      "11번째 학습 - 배치 : 156\n",
      "11번째 학습 - 배치 : 157\n",
      "11번째 학습 - 배치 : 158\n",
      "11번째 학습 - 배치 : 159\n",
      "11번째 학습 - 배치 : 160\n",
      "11번째 학습 - 배치 : 161\n",
      "11번째 학습 - 배치 : 162\n",
      "11번째 학습 - 배치 : 163\n",
      "11번째 학습 - 배치 : 164\n",
      "11번째 학습 - 배치 : 165\n",
      "11번째 학습 - 배치 : 166\n",
      "11번째 학습 - 배치 : 167\n",
      "11번째 학습 - 배치 : 168\n",
      "11번째 학습 - 배치 : 169\n",
      "11번째 학습 - 배치 : 170\n",
      "11번째 학습 - 배치 : 171\n",
      "11번째 학습 - 배치 : 172\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-366-670ff022a20c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malready\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_o\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_p\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mclear_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-364-14cd8a5bc491>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, optimizer, epoch)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{0}번째 학습 - 배치 : {1}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-360-2b775a873c7d>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     15\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-316-098b1f2e6a94>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavg_pool2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-316-098b1f2e6a94>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshortcut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1061\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1062\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1063\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1064\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "already=1\n",
    "for epoch in range(already, EPOCHS + 1):\n",
    "    scheduler.step()\n",
    "    train(model, train_loader, optimizer, epoch)\n",
    "    test_loss, test_accuracy, _o, _p, _t = evaluate(model, test_loader)\n",
    "    clear_output(wait=True)\n",
    "    print(\"아웃풋 : \" + str(_o))\n",
    "    print(\"모방 모델의 답안 : \" + str(_p))\n",
    "    print(\"원본 모델의 답안 : \" + str(_t))\n",
    "    print('[{}] Test Loss: {:.4f}, Accuracy: {:.2f}%'.format(\n",
    "          epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (7) FGSM 이미지 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(image, epsilon, gradient):\n",
    "    sign_gradient = gradient.sign()\n",
    "    perturbed_image = image + epsilon * sign_gradient\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    return perturbed_image\n",
    "\n",
    "def getPertubedImage(img_tensor, target, epsilon, model):\n",
    "    img_tensor = img_tensor#.unsqueeze(0)\n",
    "    img_tensor.requires_grad_(True)\n",
    "    output = model(img_tensor)\n",
    "    loss = F.nll_loss(output, target) \n",
    "    \n",
    "    model.zero_grad()\n",
    "    \n",
    "    loss.requires_grad_(True)\n",
    "    loss.backward()\n",
    "    \n",
    "    gradient = img_tensor.grad.data\n",
    "    perturbed_data = fgsm_attack(img_tensor, epsilon, gradient)\n",
    "    return perturbed_data#.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모범 답안 : 고양이\n",
      "----------------------------------------------------------------------------------------------------\n",
      "원본 이미지 원본 모델 예측 : 고양이\n",
      "원본 이미지 모방 모델 예측 : 고양이\n",
      "----------------------------------------------------------------------------------------------------\n",
      "왜곡 이미지(타겟 = real) 원본 모델 예측 : 배\n",
      "왜곡 이미지(타겟 = real) 모방 모델 예측 : 배\n",
      "----------------------------------------------------------------------------------------------------\n",
      "왜곡 이미지(타겟 = 원본 모델) 원본 모델 예측 : 배\n",
      "왜곡 이미지(타겟 = 원본 모델) 모방 모델 예측 : 배\n",
      "----------------------------------------------------------------------------------------------------\n",
      "왜곡 이미지(타겟 = 모방 모델) 원본 모델 예측 : 배\n",
      "왜곡 이미지(타겟 = 모방 모델) 모방 모델 예측 : 배\n"
     ]
    }
   ],
   "source": [
    "class_labels = [\n",
    "    \"비행기\",\n",
    "    \"자동차\",\n",
    "    \"새\",\n",
    "    \"고양이\",\n",
    "    \"사슴\",\n",
    "    \"개\",\n",
    "    \"개구리\",\n",
    "    \"말\",\n",
    "    \"배\",\n",
    "    \"트럭\"\n",
    "]\n",
    "epsilon = 0.2\n",
    "\n",
    "for data, target in test_cifar_loader:\n",
    "    if target.item()!=willBeHackedModel(data).max(1, keepdim=True)[1].item() or target.item()!=model(data).max(1, keepdim=True)[1].item() :\n",
    "        continue\n",
    "    pic = np.transpose(data.detach().numpy()[0], (1, 2, 0))\n",
    "    plt.imshow(pic)\n",
    "    plt.show()\n",
    "    print(\"모범 답안 : \" + str(class_labels[target.item()]))\n",
    "    print(\"----------------------------------------------------------------------------------------------------\")\n",
    "    print(\"원본 이미지 원본 모델 예측 : \" + str(class_labels[willBeHackedModel(data).max(1, keepdim=True)[1].item()]))\n",
    "    print(\"원본 이미지 모방 모델 예측 : \" + str(class_labels[model(data).max(1, keepdim=True)[1].item()]))\n",
    "    print(\"----------------------------------------------------------------------------------------------------\")\n",
    "    \n",
    "    realTarget = target\n",
    "    perturbed_image = getPertubedImage(data, realTarget, epsilon, model)\n",
    "    print(\"왜곡 이미지(타겟 = real) 원본 모델 예측 : \" + str(class_labels[willBeHackedModel(perturbed_image).max(1, keepdim=True)[1].item()]))\n",
    "    print(\"왜곡 이미지(타겟 = real) 모방 모델 예측 : \" + str(class_labels[model(perturbed_image).max(1, keepdim=True)[1].item()]))\n",
    "    print(\"----------------------------------------------------------------------------------------------------\")\n",
    "    originalTarget = willBeHackedModel(data).max(1, keepdim=True)[1].squeeze(0)\n",
    "    perturbed_image = getPertubedImage(data, originalTarget, epsilon, model)\n",
    "    print(\"왜곡 이미지(타겟 = 원본 모델) 원본 모델 예측 : \" + str(class_labels[willBeHackedModel(perturbed_image).max(1, keepdim=True)[1].item()]))\n",
    "    print(\"왜곡 이미지(타겟 = 원본 모델) 모방 모델 예측 : \" + str(class_labels[model(perturbed_image).max(1, keepdim=True)[1].item()]))\n",
    "    print(\"----------------------------------------------------------------------------------------------------\")\n",
    "    imitaionTarget = model(data).max(1, keepdim=True)[1].squeeze(0)\n",
    "    perturbed_image = getPertubedImage(data, imitaionTarget, epsilon, model)\n",
    "    print(\"왜곡 이미지(타겟 = 모방 모델) 원본 모델 예측 : \" + str(class_labels[willBeHackedModel(perturbed_image).max(1, keepdim=True)[1].item()]))\n",
    "    print(\"왜곡 이미지(타겟 = 모방 모델) 모방 모델 예측 : \" + str(class_labels[model(perturbed_image).max(1, keepdim=True)[1].item()]))\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(model(perturbed_image).data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LHZ\\anaconda3\\lib\\site-packages\\torch\\serialization.py:402: UserWarning: Couldn't retrieve source code for container of type ImitationResNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\LHZ\\anaconda3\\lib\\site-packages\\torch\\serialization.py:402: UserWarning: Couldn't retrieve source code for container of type BasicBlock. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, './model/Attack Model/model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
